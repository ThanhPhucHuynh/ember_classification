{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"Data_Processing.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"fYdYQA862LSp"},"source":["#  download tập dataset Ember\n","!wget https://ember.elastic.co/ember_dataset_2017_2.tar.bz2 --no-check-certificate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B3WlJqWN3vr7"},"source":["# Decompressing a .bz2 file\n","!bzip2 -d ember_dataset_2017_2.tar.bz2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F2sP5Vdh5X7M"},"source":["# Extracting from tar file\n","!tar -xvf ember_dataset_2017_2.tar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzUN-QyRVjhD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639234329421,"user_tz":-420,"elapsed":9861,"user":{"displayName":"Le Van Son B1706862","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05760697644039183687"}},"outputId":"067d785f-4190-4395-8018-2fe59507c61d"},"source":["pip install lief"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lief\n","  Downloading lief-0.11.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n","\u001b[K     |████████████████████████████████| 3.9 MB 5.3 MB/s \n","\u001b[?25hInstalling collected packages: lief\n","Successfully installed lief-0.11.5\n"]}]},{"cell_type":"code","source":["\n","import re\n","import lief\n","import hashlib\n","import numpy as np\n","from sklearn.feature_extraction import FeatureHasher\n","\n","LIEF_MAJOR, LIEF_MINOR, _ = lief.__version__.split('.')\n","LIEF_EXPORT_OBJECT = int(LIEF_MAJOR) > 0 or ( int(LIEF_MAJOR)==0 and int(LIEF_MINOR) >= 10 )\n","LIEF_HAS_SIGNATURE = int(LIEF_MAJOR) > 0 or (int(LIEF_MAJOR) == 0 and int(LIEF_MINOR) >= 11)\n","\n","\n","class FeatureType(object):\n","    ''' Base class from which each feature type may inherit '''\n","\n","    name = ''\n","    dim = 0\n","\n","    def __repr__(self):\n","        return '{}({})'.format(self.name, self.dim)\n","\n","    def raw_features(self, bytez, lief_binary):\n","        ''' Generate a JSON-able representation of the file '''\n","        raise (NotImplementedError)\n","\n","    def process_raw_features(self, raw_obj):\n","        ''' Generate a feature vector from the raw features '''\n","        raise (NotImplementedError)\n","\n","    def feature_vector(self, bytez, lief_binary):\n","        ''' Directly calculate the feature vector from the sample itself. This should only be implemented differently\n","        if there are significant speedups to be gained from combining the two functions. '''\n","        return self.process_raw_features(self.raw_features(bytez, lief_binary))\n","\n","\n","class ByteHistogram(FeatureType):\n","    ''' Byte histogram (count + non-normalized) over the entire binary file '''\n","\n","    name = 'histogram'\n","    dim = 256\n","\n","    def __init__(self):\n","        super(FeatureType, self).__init__()\n","\n","    def raw_features(self, bytez, lief_binary):\n","        counts = np.bincount(np.frombuffer(bytez, dtype=np.uint8), minlength=256)\n","        return counts.tolist()\n","\n","    def process_raw_features(self, raw_obj):\n","        counts = np.array(raw_obj, dtype=np.float32)\n","        sum = counts.sum()\n","        normalized = counts / sum\n","        return normalized\n","\n","\n","class ByteEntropyHistogram(FeatureType):\n","    ''' 2d byte/entropy histogram based loosely on (Saxe and Berlin, 2015).\n","    This roughly approximates the joint probability of byte value and local entropy.\n","    See Section 2.1.1 in https://arxiv.org/pdf/1508.03096.pdf for more info.\n","    '''\n","\n","    name = 'byteentropy'\n","    dim = 256\n","\n","    def __init__(self, step=1024, window=2048):\n","        super(FeatureType, self).__init__()\n","        self.window = window\n","        self.step = step\n","\n","    def _entropy_bin_counts(self, block):\n","        # coarse histogram, 16 bytes per bin\n","        c = np.bincount(block >> 4, minlength=16)  # 16-bin histogram\n","        p = c.astype(np.float32) / self.window\n","        wh = np.where(c)[0]\n","        H = np.sum(-p[wh] * np.log2(\n","            p[wh])) * 2  # * x2 b.c. we reduced information by half: 256 bins (8 bits) to 16 bins (4 bits)\n","\n","        Hbin = int(H * 2)  # up to 16 bins (max entropy is 8 bits)\n","        if Hbin == 16:  # handle entropy = 8.0 bits\n","            Hbin = 15\n","\n","        return Hbin, c\n","\n","    def raw_features(self, bytez, lief_binary):\n","        output = np.zeros((16, 16), dtype=np.int)\n","        a = np.frombuffer(bytez, dtype=np.uint8)\n","        if a.shape[0] < self.window:\n","            Hbin, c = self._entropy_bin_counts(a)\n","            output[Hbin, :] += c\n","        else:\n","            # strided trick from here: http://www.rigtorp.se/2011/01/01/rolling-statistics-numpy.html\n","            shape = a.shape[:-1] + (a.shape[-1] - self.window + 1, self.window)\n","            strides = a.strides + (a.strides[-1],)\n","            blocks = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)[::self.step, :]\n","\n","            # from the blocks, compute histogram\n","            for block in blocks:\n","                Hbin, c = self._entropy_bin_counts(block)\n","                output[Hbin, :] += c\n","\n","        return output.flatten().tolist()\n","\n","    def process_raw_features(self, raw_obj):\n","        counts = np.array(raw_obj, dtype=np.float32)\n","        sum = counts.sum()\n","        normalized = counts / sum\n","        return normalized\n","\n","\n","class SectionInfo(FeatureType):\n","    ''' Information about section names, sizes and entropy.  Uses hashing trick\n","    to summarize all this section info into a feature vector.\n","    '''\n","\n","    name = 'section'\n","    dim = 5 + 50 + 50 + 50 + 50 + 50\n","\n","    def __init__(self):\n","        super(FeatureType, self).__init__()\n","\n","    @staticmethod\n","    def _properties(s):\n","        return [str(c).split('.')[-1] for c in s.characteristics_lists]\n","\n","    def raw_features(self, bytez, lief_binary):\n","        if lief_binary is None:\n","            return {\"entry\": \"\", \"sections\": []}\n","\n","        # properties of entry point, or if invalid, the first executable section\n","        try:\n","            entry_section = lief_binary.section_from_offset(lief_binary.entrypoint).name\n","        except lief.not_found:\n","            # bad entry point, let's find the first executable section\n","            entry_section = \"\"\n","            for s in lief_binary.sections:\n","                if lief.PE.SECTION_CHARACTERISTICS.MEM_EXECUTE in s.characteristics_lists:\n","                    entry_section = s.name\n","                    break\n","\n","        raw_obj = {\"entry\": entry_section}\n","        raw_obj[\"sections\"] = [{\n","            'name': s.name,\n","            'size': s.size,\n","            'entropy': s.entropy,\n","            'vsize': s.virtual_size,\n","            'props': self._properties(s)\n","        } for s in lief_binary.sections]\n","        return raw_obj\n","\n","    def process_raw_features(self, raw_obj):\n","        sections = raw_obj['sections']\n","        general = [\n","            len(sections),  # total number of sections\n","            # number of sections with nonzero size\n","            sum(1 for s in sections if s['size'] == 0),\n","            # number of sections with an empty name\n","            sum(1 for s in sections if s['name'] == \"\"),\n","            # number of RX\n","            sum(1 for s in sections if 'MEM_READ' in s['props'] and 'MEM_EXECUTE' in s['props']),\n","            # number of W\n","            sum(1 for s in sections if 'MEM_WRITE' in s['props'])\n","        ]\n","        # gross characteristics of each section\n","        section_sizes = [(s['name'], s['size']) for s in sections]\n","        section_sizes_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_sizes]).toarray()[0]\n","        section_entropy = [(s['name'], s['entropy']) for s in sections]\n","        section_entropy_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_entropy]).toarray()[0]\n","        section_vsize = [(s['name'], s['vsize']) for s in sections]\n","        section_vsize_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_vsize]).toarray()[0]\n","        entry_name_hashed = FeatureHasher(50, input_type=\"string\").transform([raw_obj['entry']]).toarray()[0]\n","        characteristics = [p for s in sections for p in s['props'] if s['name'] == raw_obj['entry']]\n","        characteristics_hashed = FeatureHasher(50, input_type=\"string\").transform([characteristics]).toarray()[0]\n","\n","        return np.hstack([\n","            general, section_sizes_hashed, section_entropy_hashed, section_vsize_hashed, entry_name_hashed,\n","            characteristics_hashed\n","        ]).astype(np.float32)\n","\n","\n","class ImportsInfo(FeatureType):\n","    ''' Information about imported libraries and functions from the\n","    import address table.  Note that the total number of imported\n","    functions is contained in GeneralFileInfo.\n","    '''\n","\n","    name = 'imports'\n","    dim = 1280\n","\n","    def __init__(self):\n","        super(FeatureType, self).__init__()\n","\n","    def raw_features(self, bytez, lief_binary):\n","        imports = {}\n","        if lief_binary is None:\n","            return imports\n","\n","        for lib in lief_binary.imports:\n","            if lib.name not in imports:\n","                imports[lib.name] = []  # libraries can be duplicated in listing, extend instead of overwrite\n","\n","            # Clipping assumes there are diminishing returns on the discriminatory power of imported functions\n","            #  beyond the first 10000 characters, and this will help limit the dataset size\n","            for entry in lib.entries:\n","                if entry.is_ordinal:\n","                    imports[lib.name].append(\"ordinal\" + str(entry.ordinal))\n","                else:\n","                    imports[lib.name].append(entry.name[:10000])\n","\n","        return imports\n","\n","    def process_raw_features(self, raw_obj):\n","        # unique libraries\n","        libraries = list(set([l.lower() for l in raw_obj.keys()]))\n","        libraries_hashed = FeatureHasher(256, input_type=\"string\").transform([libraries]).toarray()[0]\n","\n","        # A string like \"kernel32.dll:CreateFileMappingA\" for each imported function\n","        imports = [lib.lower() + ':' + e for lib, elist in raw_obj.items() for e in elist]\n","        imports_hashed = FeatureHasher(1024, input_type=\"string\").transform([imports]).toarray()[0]\n","\n","        # Two separate elements: libraries (alone) and fully-qualified names of imported functions\n","        return np.hstack([libraries_hashed, imports_hashed]).astype(np.float32)\n","\n","\n","class ExportsInfo(FeatureType):\n","    ''' Information about exported functions. Note that the total number of exported\n","    functions is contained in GeneralFileInfo.\n","    '''\n","\n","    name = 'exports'\n","    dim = 128\n","\n","    def __init__(self):\n","        super(FeatureType, self).__init__()\n","\n","    def raw_features(self, bytez, lief_binary):\n","        if lief_binary is None:\n","            return []\n","\n","        # Clipping assumes there are diminishing returns on the discriminatory power of exports beyond\n","        #  the first 10000 characters, and this will help limit the dataset size\n","        if LIEF_EXPORT_OBJECT:\n","            # export is an object with .name attribute (0.10.0 and later)\n","            clipped_exports = [export.name[:10000] for export in lief_binary.exported_functions]\n","        else:\n","            # export is a string (LIEF 0.9.0 and earlier)\n","            clipped_exports = [export[:10000] for export in lief_binary.exported_functions]\n","        \n","\n","        return clipped_exports\n","\n","    def process_raw_features(self, raw_obj):\n","        exports_hashed = FeatureHasher(128, input_type=\"string\").transform([raw_obj]).toarray()[0]\n","        return exports_hashed.astype(np.float32)\n","\n","\n","class GeneralFileInfo(FeatureType):\n","    ''' General information about the file '''\n","\n","    name = 'general'\n","    dim = 10\n","\n","    def __init__(self):\n","        super(FeatureType, self).__init__()\n","\n","    def raw_features(self, bytez, lief_binary):\n","        if lief_binary is None:\n","            return {\n","                'size': len(bytez),\n","                'vsize': 0,\n","                'has_debug': 0,\n","                'exports': 0,\n","                'imports': 0,\n","                'has_relocations': 0,\n","                'has_resources': 0,\n","                'has_signature': 0,\n","                'has_tls': 0,\n","                'symbols': 0\n","            }\n","\n","        return {\n","            'size': len(bytez),\n","            'vsize': lief_binary.virtual_size,\n","            'has_debug': int(lief_binary.has_debug),\n","            'exports': len(lief_binary.exported_functions),\n","            'imports': len(lief_binary.imported_functions),\n","            'has_relocations': int(lief_binary.has_relocations),\n","            'has_resources': int(lief_binary.has_resources),\n","            'has_signature': int(lief_binary.has_signatures) if LIEF_HAS_SIGNATURE else int(lief_binary.has_signature),\n","            'has_tls': int(lief_binary.has_tls),\n","            'symbols': len(lief_binary.symbols),\n","        }\n","\n","    def process_raw_features(self, raw_obj):\n","        return np.asarray([\n","            raw_obj['size'], raw_obj['vsize'], raw_obj['has_debug'], raw_obj['exports'], raw_obj['imports'],\n","            raw_obj['has_relocations'], raw_obj['has_resources'], raw_obj['has_signature'], raw_obj['has_tls'],\n","            raw_obj['symbols']\n","        ],\n","                          dtype=np.float32)\n","\n","\n","class HeaderFileInfo(FeatureType):\n","    ''' Machine, architecure, OS, linker and other information extracted from header '''\n","\n","    name = 'header'\n","    dim = 62\n","\n","    def __init__(self):\n","        super(FeatureType, self).__init__()\n","\n","    def raw_features(self, bytez, lief_binary):\n","        raw_obj = {}\n","        raw_obj['coff'] = {'timestamp': 0, 'machine': \"\", 'characteristics': []}\n","        raw_obj['optional'] = {\n","            'subsystem': \"\",\n","            'dll_characteristics': [],\n","            'magic': \"\",\n","            'major_image_version': 0,\n","            'minor_image_version': 0,\n","            'major_linker_version': 0,\n","            'minor_linker_version': 0,\n","            'major_operating_system_version': 0,\n","            'minor_operating_system_version': 0,\n","            'major_subsystem_version': 0,\n","            'minor_subsystem_version': 0,\n","            'sizeof_code': 0,\n","            'sizeof_headers': 0,\n","            'sizeof_heap_commit': 0\n","        }\n","        if lief_binary is None:\n","            return raw_obj\n","\n","        raw_obj['coff']['timestamp'] = lief_binary.header.time_date_stamps\n","        raw_obj['coff']['machine'] = str(lief_binary.header.machine).split('.')[-1]\n","        raw_obj['coff']['characteristics'] = [str(c).split('.')[-1] for c in lief_binary.header.characteristics_list]\n","        raw_obj['optional']['subsystem'] = str(lief_binary.optional_header.subsystem).split('.')[-1]\n","        raw_obj['optional']['dll_characteristics'] = [\n","            str(c).split('.')[-1] for c in lief_binary.optional_header.dll_characteristics_lists\n","        ]\n","        raw_obj['optional']['magic'] = str(lief_binary.optional_header.magic).split('.')[-1]\n","        raw_obj['optional']['major_image_version'] = lief_binary.optional_header.major_image_version\n","        raw_obj['optional']['minor_image_version'] = lief_binary.optional_header.minor_image_version\n","        raw_obj['optional']['major_linker_version'] = lief_binary.optional_header.major_linker_version\n","        raw_obj['optional']['minor_linker_version'] = lief_binary.optional_header.minor_linker_version\n","        raw_obj['optional'][\n","            'major_operating_system_version'] = lief_binary.optional_header.major_operating_system_version\n","        raw_obj['optional'][\n","            'minor_operating_system_version'] = lief_binary.optional_header.minor_operating_system_version\n","        raw_obj['optional']['major_subsystem_version'] = lief_binary.optional_header.major_subsystem_version\n","        raw_obj['optional']['minor_subsystem_version'] = lief_binary.optional_header.minor_subsystem_version\n","        raw_obj['optional']['sizeof_code'] = lief_binary.optional_header.sizeof_code\n","        raw_obj['optional']['sizeof_headers'] = lief_binary.optional_header.sizeof_headers\n","        raw_obj['optional']['sizeof_heap_commit'] = lief_binary.optional_header.sizeof_heap_commit\n","        return raw_obj\n","\n","    def process_raw_features(self, raw_obj):\n","        return np.hstack([\n","            raw_obj['coff']['timestamp'],\n","            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['coff']['machine']]]).toarray()[0],\n","            FeatureHasher(10, input_type=\"string\").transform([raw_obj['coff']['characteristics']]).toarray()[0],\n","            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['optional']['subsystem']]]).toarray()[0],\n","            FeatureHasher(10, input_type=\"string\").transform([raw_obj['optional']['dll_characteristics']]).toarray()[0],\n","            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['optional']['magic']]]).toarray()[0],\n","            raw_obj['optional']['major_image_version'],\n","            raw_obj['optional']['minor_image_version'],\n","            raw_obj['optional']['major_linker_version'],\n","            raw_obj['optional']['minor_linker_version'],\n","            raw_obj['optional']['major_operating_system_version'],\n","            raw_obj['optional']['minor_operating_system_version'],\n","            raw_obj['optional']['major_subsystem_version'],\n","            raw_obj['optional']['minor_subsystem_version'],\n","            raw_obj['optional']['sizeof_code'],\n","            raw_obj['optional']['sizeof_headers'],\n","            raw_obj['optional']['sizeof_heap_commit'],\n","        ]).astype(np.float32)\n","\n","\n","class StringExtractor(FeatureType):\n","    ''' Extracts strings from raw byte stream '''\n","\n","    name = 'strings'\n","    dim = 1 + 1 + 1 + 96 + 1 + 1 + 1 + 1 + 1\n","\n","    def __init__(self):\n","        super(FeatureType, self).__init__()\n","        # all consecutive runs of 0x20 - 0x7f that are 5+ characters\n","        self._allstrings = re.compile(b'[\\x20-\\x7f]{5,}')\n","        # occurances of the string 'C:\\'.  Not actually extracting the path\n","        self._paths = re.compile(b'c:\\\\\\\\', re.IGNORECASE)\n","        # occurances of http:// or https://.  Not actually extracting the URLs\n","        self._urls = re.compile(b'https?://', re.IGNORECASE)\n","        # occurances of the string prefix HKEY_.  No actually extracting registry names\n","        self._registry = re.compile(b'HKEY_')\n","        # crude evidence of an MZ header (dropper?) somewhere in the byte stream\n","        self._mz = re.compile(b'MZ')\n","\n","    def raw_features(self, bytez, lief_binary):\n","        allstrings = self._allstrings.findall(bytez)\n","        if allstrings:\n","            # statistics about strings:\n","            string_lengths = [len(s) for s in allstrings]\n","            avlength = sum(string_lengths) / len(string_lengths)\n","            # map printable characters 0x20 - 0x7f to an int array consisting of 0-95, inclusive\n","            as_shifted_string = [b - ord(b'\\x20') for b in b''.join(allstrings)]\n","            c = np.bincount(as_shifted_string, minlength=96)  # histogram count\n","            # distribution of characters in printable strings\n","            csum = c.sum()\n","            p = c.astype(np.float32) / csum\n","            wh = np.where(c)[0]\n","            H = np.sum(-p[wh] * np.log2(p[wh]))  # entropy\n","        else:\n","            avlength = 0\n","            c = np.zeros((96,), dtype=np.float32)\n","            H = 0\n","            csum = 0\n","\n","        return {\n","            'numstrings': len(allstrings),\n","            'avlength': avlength,\n","            'printabledist': c.tolist(),  # store non-normalized histogram\n","            'printables': int(csum),\n","            'entropy': float(H),\n","            'paths': len(self._paths.findall(bytez)),\n","            'urls': len(self._urls.findall(bytez)),\n","            'registry': len(self._registry.findall(bytez)),\n","            'MZ': len(self._mz.findall(bytez))\n","        }\n","\n","    def process_raw_features(self, raw_obj):\n","        hist_divisor = float(raw_obj['printables']) if raw_obj['printables'] > 0 else 1.0\n","        return np.hstack([\n","            raw_obj['numstrings'], raw_obj['avlength'], raw_obj['printables'],\n","            np.asarray(raw_obj['printabledist']) / hist_divisor, raw_obj['entropy'], raw_obj['paths'], raw_obj['urls'],\n","            raw_obj['registry'], raw_obj['MZ']\n","        ]).astype(np.float32)\n","\n","\n","class DataDirectories(FeatureType):\n","    ''' Extracts size and virtual address of the first 15 data directories '''\n","\n","    name = 'datadirectories'\n","    dim = 15 * 2\n","\n","    def __init__(self):\n","        super(FeatureType, self).__init__()\n","        self._name_order = [\n","            \"EXPORT_TABLE\", \"IMPORT_TABLE\", \"RESOURCE_TABLE\", \"EXCEPTION_TABLE\", \"CERTIFICATE_TABLE\",\n","            \"BASE_RELOCATION_TABLE\", \"DEBUG\", \"ARCHITECTURE\", \"GLOBAL_PTR\", \"TLS_TABLE\", \"LOAD_CONFIG_TABLE\",\n","            \"BOUND_IMPORT\", \"IAT\", \"DELAY_IMPORT_DESCRIPTOR\", \"CLR_RUNTIME_HEADER\"\n","        ]\n","\n","    def raw_features(self, bytez, lief_binary):\n","        output = []\n","        if lief_binary is None:\n","            return output\n","\n","        for data_directory in lief_binary.data_directories:\n","            output.append({\n","                \"name\": str(data_directory.type).replace(\"DATA_DIRECTORY.\", \"\"),\n","                \"size\": data_directory.size,\n","                \"virtual_address\": data_directory.rva\n","            })\n","        return output\n","\n","    def process_raw_features(self, raw_obj):\n","        features = np.zeros(2 * len(self._name_order), dtype=np.float32)\n","        for i in range(len(self._name_order)):\n","            if i < len(raw_obj):\n","                features[2 * i] = raw_obj[i][\"size\"]\n","                features[2 * i + 1] = raw_obj[i][\"virtual_address\"]\n","        return features\n","\n","\n","class PEFeatureExtractor(object):\n","    ''' Extract useful features from a PE file, and return as a vector of fixed size. '''\n","\n","    def __init__(self, feature_version=2, print_feature_warning=True):\n","        self.features = [\n","            ByteHistogram(),\n","            ByteEntropyHistogram(),\n","            StringExtractor(),\n","            GeneralFileInfo(),\n","            HeaderFileInfo(),\n","            SectionInfo(),\n","            ImportsInfo(),\n","            ExportsInfo()\n","        ]\n","        if feature_version == 1:\n","            if not lief.__version__.startswith(\"0.8.3\"):\n","                if print_feature_warning:\n","                    print(f\"WARNING: EMBER feature version 1 were computed using lief version 0.8.3-18d5b75\")\n","                    print(f\"WARNING:   lief version {lief.__version__} found instead. There may be slight inconsistencies\")\n","                    print(f\"WARNING:   in the feature calculations.\")\n","        elif feature_version == 2:\n","            self.features.append(DataDirectories())\n","            if not lief.__version__.startswith(\"0.9.0\"):\n","                if print_feature_warning:\n","                    print(f\"WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\")\n","                    print(f\"WARNING:   lief version {lief.__version__} found instead. There may be slight inconsistencies\")\n","                    print(f\"WARNING:   in the feature calculations.\")\n","        else:\n","            raise Exception(f\"EMBER feature version must be 1 or 2. Not {feature_version}\")\n","        self.dim = sum([fe.dim for fe in self.features])\n","\n","    def raw_features(self, bytez):\n","        lief_errors = (lief.bad_format, lief.bad_file, lief.pe_error, lief.parser_error, lief.read_out_of_bound,\n","                       RuntimeError)\n","        try:\n","            lief_binary = lief.PE.parse(list(bytez))\n","        except lief_errors as e:\n","            print(\"lief error: \", str(e))\n","            lief_binary = None\n","        except Exception:  # everything else (KeyboardInterrupt, SystemExit, ValueError):\n","            raise\n","\n","        features = {\"sha256\": hashlib.sha256(bytez).hexdigest()}\n","        features.update({fe.name: fe.raw_features(bytez, lief_binary) for fe in self.features})\n","        return features\n","\n","    def process_raw_features(self, raw_obj):\n","        feature_vectors = [fe.process_raw_features(raw_obj[fe.name]) for fe in self.features]\n","        return np.hstack(feature_vectors).astype(np.float32)\n","\n","    def feature_vector(self, bytez):\n","        return self.process_raw_features(self.raw_features(bytez))"],"metadata":{"id":"JTP_2X7b807y","executionInfo":{"status":"ok","timestamp":1639234331804,"user_tz":-420,"elapsed":2396,"user":{"displayName":"Le Van Son B1706862","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05760697644039183687"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"WoSzjAVofiQX"},"source":["import re\n","import lief\n","import hashlib\n","import numpy as np\n","from sklearn.feature_extraction import FeatureHasher\n","\n","LIEF_MAJOR, LIEF_MINOR, _ = lief.__version__.split('.')\n","LIEF_EXPORT_OBJECT = int(LIEF_MAJOR) > 0 or ( int(LIEF_MAJOR)==0 and int(LIEF_MINOR) >= 10 )\n","LIEF_HAS_SIGNATURE = int(LIEF_MAJOR) > 0 or (int(LIEF_MAJOR) == 0 and int(LIEF_MINOR) >= 11)\n","\n","\n","class FeatureType(object):\n","    ''' Base class from which each feature type may inherit '''\n","\n","    name = ''\n","    dim = 0\n","\n","    def __repr__(self):\n","        return '{}({})'.format(self.name, self.dim)\n","\n","    def raw_features(self, bytez, lief_binary):\n","        ''' Generate a JSON-able representation of the file '''\n","        raise (NotImplementedError)\n","\n","    def process_raw_features(self, raw_obj):\n","        ''' Generate a feature vector from the raw features '''\n","        raise (NotImplementedError)\n","\n","    def feature_vector(self, bytez, lief_binary):\n","        ''' Directly calculate the feature vector from the sample itself. This should only be implemented differently\n","        if there are significant speedups to be gained from combining the two functions. '''\n","        return self.process_raw_features(self.raw_features(bytez, lief_binary))\n","\n","class ByteHistogram(FeatureType):\n","    ''' Byte histogram (count + non-normalized) over the entire binary file '''\n","\n","    name = 'histogram'\n","    dim = 256\n","\n","    def __init__(self):\n","        super(FeatureType, self).__init__()\n","\n","    def raw_features(self, bytez, lief_binary):\n","        counts = np.bincount(np.frombuffer(bytez, dtype=np.uint8), minlength=256)\n","        return counts.tolist()\n","\n","    def process_raw_features(self, raw_obj):\n","        counts = np.array(raw_obj, dtype=np.float32)\n","        sum = counts.sum()\n","        normalized = counts / sum\n","        return normalized\n","\n","class ByteEntropyHistogram(FeatureType):\n","    ''' 2d byte/entropy histogram based loosely on (Saxe and Berlin, 2015).\n","    This roughly approximates the joint probability of byte value and local entropy.\n","    See Section 2.1.1 in https://arxiv.org/pdf/1508.03096.pdf for more info.\n","    '''\n","\n","    name = 'byteentropy'\n","    dim = 256\n","\n","    def __init__(self, step=1024, window=2048):\n","        super(FeatureType, self).__init__()\n","        self.window = window\n","        self.step = step\n","\n","    def _entropy_bin_counts(self, block):\n","        # coarse histogram, 16 bytes per bin\n","        c = np.bincount(block >> 4, minlength=16)  # 16-bin histogram\n","        p = c.astype(np.float32) / self.window\n","        wh = np.where(c)[0]\n","        H = np.sum(-p[wh] * np.log2(\n","            p[wh])) * 2  # * x2 b.c. we reduced information by half: 256 bins (8 bits) to 16 bins (4 bits)\n","\n","        Hbin = int(H * 2)  # up to 16 bins (max entropy is 8 bits)\n","        if Hbin == 16:  # handle entropy = 8.0 bits\n","            Hbin = 15\n","\n","        return Hbin, c\n","\n","    def raw_features(self, bytez, lief_binary):\n","        output = np.zeros((16, 16), dtype=np.int)\n","        a = np.frombuffer(bytez, dtype=np.uint8)\n","        if a.shape[0] < self.window:\n","            Hbin, c = self._entropy_bin_counts(a)\n","            output[Hbin, :] += c\n","        else:\n","            # strided trick from here: http://www.rigtorp.se/2011/01/01/rolling-statistics-numpy.html\n","            shape = a.shape[:-1] + (a.shape[-1] - self.window + 1, self.window)\n","            strides = a.strides + (a.strides[-1],)\n","            blocks = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)[::self.step, :]\n","\n","            # from the blocks, compute histogram\n","            for block in blocks:\n","                Hbin, c = self._entropy_bin_counts(block)\n","                output[Hbin, :] += c\n","\n","        return output.flatten().tolist()\n","\n","    def process_raw_features(self, raw_obj):\n","        counts = np.array(raw_obj, dtype=np.float32)\n","        sum = counts.sum()\n","        normalized = counts / sum\n","        return normalized\n","\n","class SectionInfo(FeatureType):\n","    ''' Information about section names, sizes and entropy.  Uses hashing trick\n","    to summarize all this section info into a feature vector.\n","    '''\n","\n","    name = 'section'\n","    dim = 5 + 50 + 50 + 50 + 50 + 50\n","\n","    def __init__(self):\n","        super(FeatureType, self).__init__()\n","\n","    @staticmethod\n","    def _properties(s):\n","        return [str(c).split('.')[-1] for c in s.characteristics_lists]\n","\n","    def raw_features(self, bytez, lief_binary):\n","        if lief_binary is None:\n","            return {\"entry\": \"\", \"sections\": []}\n","\n","        # properties of entry point, or if invalid, the first executable section\n","        try:\n","            entry_section = lief_binary.section_from_offset(lief_binary.entrypoint).name\n","        except lief.not_found:\n","            # bad entry point, let's find the first executable section\n","            entry_section = \"\"\n","            for s in lief_binary.sections:\n","                if lief.PE.SECTION_CHARACTERISTICS.MEM_EXECUTE in s.characteristics_lists:\n","                    entry_section = s.name\n","                    break\n","\n","        raw_obj = {\"entry\": entry_section}\n","        raw_obj[\"sections\"] = [{\n","            'name': s.name,\n","            'size': s.size,\n","            'entropy': s.entropy,\n","            'vsize': s.virtual_size,\n","            'props': self._properties(s)\n","        } for s in lief_binary.sections]\n","        return raw_obj\n","\n","    def process_raw_features(self, raw_obj):\n","        sections = raw_obj['sections']\n","        general = [\n","            len(sections),  # total number of sections\n","            # number of sections with nonzero size\n","            sum(1 for s in sections if s['size'] == 0),\n","            # number of sections with an empty name\n","            sum(1 for s in sections if s['name'] == \"\"),\n","            # number of RX\n","            sum(1 for s in sections if 'MEM_READ' in s['props'] and 'MEM_EXECUTE' in s['props']),\n","            # number of W\n","            sum(1 for s in sections if 'MEM_WRITE' in s['props'])\n","        ]\n","        # gross characteristics of each section\n","        section_sizes = [(s['name'], s['size']) for s in sections]\n","        section_sizes_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_sizes]).toarray()[0]\n","        section_entropy = [(s['name'], s['entropy']) for s in sections]\n","        section_entropy_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_entropy]).toarray()[0]\n","        section_vsize = [(s['name'], s['vsize']) for s in sections]\n","        section_vsize_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_vsize]).toarray()[0]\n","        entry_name_hashed = FeatureHasher(50, input_type=\"string\").transform([raw_obj['entry']]).toarray()[0]\n","        characteristics = [p for s in sections for p in s['props'] if s['name'] == raw_obj['entry']]\n","        characteristics_hashed = FeatureHasher(50, input_type=\"string\").transform([characteristics]).toarray()[0]\n","\n","        return np.hstack([\n","            general, section_sizes_hashed, section_entropy_hashed, section_vsize_hashed, entry_name_hashed,\n","            characteristics_hashed\n","        ]).astype(np.float32)\n","\n","class DataDirectories(FeatureType):\n","    ''' Extracts size and virtual address of the first 15 data directories '''\n","\n","    name = 'datadirectories'\n","    dim = 15 * 2\n","\n","    def __init__(self):\n","        super(FeatureType, self).__init__()\n","        self._name_order = [\n","            \"EXPORT_TABLE\", \"IMPORT_TABLE\", \"RESOURCE_TABLE\", \"EXCEPTION_TABLE\", \"CERTIFICATE_TABLE\",\n","            \"BASE_RELOCATION_TABLE\", \"DEBUG\", \"ARCHITECTURE\", \"GLOBAL_PTR\", \"TLS_TABLE\", \"LOAD_CONFIG_TABLE\",\n","            \"BOUND_IMPORT\", \"IAT\", \"DELAY_IMPORT_DESCRIPTOR\", \"CLR_RUNTIME_HEADER\"\n","        ]\n","\n","    def raw_features(self, bytez, lief_binary):\n","        output = []\n","        if lief_binary is None:\n","            return output\n","\n","        for data_directory in lief_binary.data_directories:\n","            output.append({\n","                \"name\": str(data_directory.type).replace(\"DATA_DIRECTORY.\", \"\"),\n","                \"size\": data_directory.size,\n","                \"virtual_address\": data_directory.rva\n","            })\n","        return output\n","\n","    def process_raw_features(self, raw_obj):\n","        features = np.zeros(2 * len(self._name_order), dtype=np.float32)\n","        for i in range(len(self._name_order)):\n","            if i < len(raw_obj):\n","                features[2 * i] = raw_obj[i][\"size\"]\n","                features[2 * i + 1] = raw_obj[i][\"virtual_address\"]\n","        return features\n","\n","class PEFeatureExtractor(object):\n","    ''' Extract useful features from a PE file, and return as a vector of fixed size. '''\n","\n","    def __init__(self, feature_version=2, print_feature_warning=True):\n","        self.features = [\n","            ByteHistogram(),\n","            ByteEntropyHistogram(),\n","            SectionInfo()\n","        ]\n","        if feature_version == 1:\n","            if not lief.__version__.startswith(\"0.8.3\"):\n","                if print_feature_warning:\n","                    print(f\"WARNING: EMBER feature version 1 were computed using lief version 0.8.3-18d5b75\")\n","                    print(f\"WARNING:   lief version {lief.__version__} found instead. There may be slight inconsistencies\")\n","                    print(f\"WARNING:   in the feature calculations.\")\n","        elif feature_version == 2:\n","            self.features.append(DataDirectories())\n","            if not lief.__version__.startswith(\"0.9.0\"):\n","                if print_feature_warning:\n","                    print(f\"WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\")\n","                    print(f\"WARNING:   lief version {lief.__version__} found instead. There may be slight inconsistencies\")\n","                    print(f\"WARNING:   in the feature calculations.\")\n","        else:\n","            raise Exception(f\"EMBER feature version must be 1 or 2. Not {feature_version}\")\n","        self.dim = sum([fe.dim for fe in self.features])\n","\n","    def raw_features(self, bytez):\n","        lief_errors = (lief.bad_format, lief.bad_file, lief.pe_error, lief.parser_error, lief.read_out_of_bound,\n","                       RuntimeError)\n","        try:\n","            lief_binary = lief.PE.parse(list(bytez))\n","        except lief_errors as e:\n","            print(\"lief error: \", str(e))\n","            lief_binary = None\n","        except Exception:  # everything else (KeyboardInterrupt, SystemExit, ValueError):\n","            raise\n","\n","        features = {\"sha256\": hashlib.sha256(bytez).hexdigest()}\n","        features.update({fe.name: fe.raw_features(bytez, lief_binary) for fe in self.features})\n","        return features\n","\n","    def process_raw_features(self, raw_obj):\n","        feature_vectors = [fe.process_raw_features(raw_obj[fe.name]) for fe in self.features]\n","        return np.hstack(feature_vectors).astype(np.float32)\n","\n","    def feature_vector(self, bytez):\n","        return self.process_raw_features(self.raw_features(bytez))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMJCwNk6FTfQ","executionInfo":{"status":"ok","timestamp":1639234345656,"user_tz":-420,"elapsed":293,"user":{"displayName":"Le Van Son B1706862","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05760697644039183687"}}},"source":["import os\n","import json\n","import tqdm\n","import numpy as np\n","import pandas as pd\n","import multiprocessing\n","\n","\"\"\"\n","  Yield raw feature strings from the inputed file paths\n","\"\"\"\n","def raw_feature_iterator(file_paths):\n","    for path in file_paths:\n","        with open(path, \"r\") as fin:\n","            for line in fin:\n","                yield line\n","\n","def vectorize_unpack(args):\n","    \"\"\"\n","    Pass through function for unpacking vectorize arguments\n","    \"\"\"\n","    return vectorize(*args)\n","\n","\n","def vectorize(irow, raw_features_string, X_path, y_path, extractor, nrows):\n","    \"\"\"\n","    Vectorize a single sample of raw features and write to a large numpy file\n","    \"\"\"\n","    raw_features = json.loads(raw_features_string)\n","    feature_vector = extractor.process_raw_features(raw_features)\n","\n","    y = np.memmap(y_path, dtype=np.float32, mode=\"r+\", shape=nrows)\n","    y[irow] = raw_features[\"label\"]\n","\n","    X = np.memmap(X_path, dtype=np.float32, mode=\"r+\", shape=(nrows, extractor.dim))\n","    X[irow] = feature_vector\n","\n","\n","def vectorize_subset(X_path, y_path, raw_feature_paths, extractor, nrows):\n","    \"\"\"\n","    Vectorize a subset of data and write it to disk\n","    \"\"\"\n","    # Create space on disk to write features to\n","    X = np.memmap(X_path, dtype=np.float32, mode=\"w+\", shape=(nrows, extractor.dim))\n","    y = np.memmap(y_path, dtype=np.float32, mode=\"w+\", shape=nrows)\n","    del X, y\n","\n","    # Distribute the vectorization work\n","    pool = multiprocessing.Pool()\n","    argument_iterator = ((irow, raw_features_string, X_path, y_path, extractor, nrows)\n","                         for irow, raw_features_string in enumerate(raw_feature_iterator(raw_feature_paths)))\n","    for _ in tqdm.tqdm(pool.imap_unordered(vectorize_unpack, argument_iterator), total=nrows):\n","        pass\n","\n","\n","def create_vectorized_features(data_dir, feature_version=2):\n","    \"\"\"\n","    Create feature vectors from raw features and write them to disk\n","    \"\"\"\n","    extractor = PEFeatureExtractor(feature_version)\n","\n","    print(\"Vectorizing training set\")\n","    X_path = os.path.join(data_dir, \"X_train.dat\")\n","    y_path = os.path.join(data_dir, \"y_train.dat\")\n","    raw_feature_paths = [os.path.join(data_dir, \"train_features_{}.jsonl\".format(i)) for i in range(6)]\n","    nrows = sum([1 for fp in raw_feature_paths for line in open(fp)])\n","    vectorize_subset(X_path, y_path, raw_feature_paths, extractor, nrows)\n","\n","    print(\"Vectorizing test set\")\n","    X_path = os.path.join(data_dir, \"X_test.dat\")\n","    y_path = os.path.join(data_dir, \"y_test.dat\")\n","    raw_feature_paths = [os.path.join(data_dir, \"test_features.jsonl\")]\n","    nrows = sum([1 for fp in raw_feature_paths for line in open(fp)])\n","    vectorize_subset(X_path, y_path, raw_feature_paths, extractor, nrows)\n","\n","\n","def read_vectorized_features(data_dir, subset=None, feature_version=2):\n","    \"\"\"\n","    Read vectorized features into memory mapped numpy arrays\n","    \"\"\"\n","    if subset is not None and subset not in [\"train\", \"test\"]:\n","        return None\n","\n","    extractor = PEFeatureExtractor(feature_version)\n","    ndim = extractor.dim\n","    X_train = None\n","    y_train = None\n","    X_test = None\n","    y_test = None\n","\n","    if subset is None or subset == \"train\":\n","        X_train_path = os.path.join(data_dir, \"X_train.dat\")\n","        y_train_path = os.path.join(data_dir, \"y_train.dat\")\n","        y_train = np.memmap(y_train_path, dtype=np.float32, mode=\"r\")\n","        N = y_train.shape[0]\n","        X_train = np.memmap(X_train_path, dtype=np.float32, mode=\"r\", shape=(N, ndim))\n","        if subset == \"train\":\n","            return X_train, y_train\n","\n","    if subset is None or subset == \"test\":\n","        X_test_path = os.path.join(data_dir, \"X_test.dat\")\n","        y_test_path = os.path.join(data_dir, \"y_test.dat\")\n","        y_test = np.memmap(y_test_path, dtype=np.float32, mode=\"r\")\n","        N = y_test.shape[0]\n","        X_test = np.memmap(X_test_path, dtype=np.float32, mode=\"r\", shape=(N, ndim))\n","        if subset == \"test\":\n","            return X_test, y_test\n","\n","    return X_train, y_train, X_test, y_test\n","             \n","\"\"\"\n","  Decode a raw features string and return the metadata fields\n","\"\"\"\n","def read_metadata_record(raw_features_string):\n","    all_data = json.loads(raw_features_string)\n","    # metadata_keys = {\"sha256\", \"label\", \"histogram\", \"byteentropy\", \"section\"}\n","    metadata_keys = {\"sha256\"}\n","    return {k: all_data[k] for k in all_data.keys() & metadata_keys}\n","\n","\n","\"\"\"\n","  Write metadata to a csv file and return its dataframe\n","\"\"\"\n","def create_metadata(data_dir):\n","    pool = multiprocessing.Pool()\n","\n","    train_feature_paths = [os.path.join(data_dir, \"train_features_{}.jsonl\".format(i)) for i in range(6)]\n","    train_records = list(pool.imap(read_metadata_record, raw_feature_iterator(train_feature_paths)))\n","\n","    # metadata_keys = [\"sha256\", \"label\", \"histogram\", \"byteentropy\", \"section\"]\n","    metadata_keys = [\"sha256\"]\n","    ordered_metadata_keys = [k for k in metadata_keys if k in train_records[0].keys()]\n","\n","    train_metadf = pd.DataFrame(train_records)[ordered_metadata_keys]\n","    train_metadf.to_csv(os.path.join(data_dir, \"train_metadata.csv\"))\n","\n","    train_records = [dict(record, **{\"subset\": \"train\"}) for record in train_records]\n","\n","    test_feature_paths = [os.path.join(data_dir, \"test_features.jsonl\")]\n","    test_records = list(pool.imap(read_metadata_record, raw_feature_iterator(test_feature_paths)))\n","\n","    test_metadf = pd.DataFrame(test_records)[ordered_metadata_keys]\n","    test_metadf.to_csv(os.path.join(data_dir, \"test_metadata.csv\"))\n","\n","    test_records = [dict(record, **{\"subset\": \"test\"}) for record in test_records]\n","\n","    all_metadata_keys = ordered_metadata_keys + [\"subset\"]\n","    metadf = pd.DataFrame(train_records + test_records)[all_metadata_keys]\n","    metadf.to_csv(os.path.join(data_dir, \"metadata.csv\"))\n","    return metadf\n","\n","\"\"\"\n","  Read an already created metadata file and return its dataframe\n","\"\"\"\n","def read_metadata(data_dir):\n","    return pd.read_csv(os.path.join(data_dir, \"metadata.csv\"), index_col=0)\n","\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"1WZidlWhZFOq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639234542963,"user_tz":-420,"elapsed":188350,"user":{"displayName":"Le Van Son B1706862","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05760697644039183687"}},"outputId":"e12230f7-6ca8-49be-aa8c-6b81066d0a4c"},"source":["# Thực hiện hàm create_vectorized_features() để tạo vector đặc trưng từ các tính năng của DL Ember\n","create_vectorized_features(\"/content/ember_2017_2/\")"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n","WARNING:   lief version 0.11.5-37bc2c9 found instead. There may be slight inconsistencies\n","WARNING:   in the feature calculations.\n","Vectorizing training set\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 900000/900000 [02:11<00:00, 6843.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Vectorizing test set\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 200000/200000 [00:34<00:00, 5752.69it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"S-T6DI-Fk4X4"},"source":["# tạo biến lưu trữ kết quả vừa thu được từ việc chuyển sang vector\n","data_features = read_vectorized_features(\"/content/ember_2017_2/\")\n","data_features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ccPxatVTrK42","executionInfo":{"status":"ok","timestamp":1639234560538,"user_tz":-420,"elapsed":290,"user":{"displayName":"Le Van Son B1706862","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05760697644039183687"}}},"source":["# lấy ra vector đặc trưng và label từ ma trận trên\n","X_train = data_features[0]\n","y_train = data_features[1].astype(int)\n","X_test = data_features[2]\n","y_test = data_features[3].astype(int)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"E1qJ3AkI4Z2s","executionInfo":{"status":"ok","timestamp":1639234571854,"user_tz":-420,"elapsed":281,"user":{"displayName":"Le Van Son B1706862","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05760697644039183687"}}},"source":["# xây dựng DataFrame mới từ vector đặc trưng và label tương ứng\n","dt_Train = pd.DataFrame(X_train)\n","dt_Train['label'] = pd.Series(y_train, index = dt_Train.index)\n","\n","dt_Test = pd.DataFrame(X_test)\n","dt_Test['label'] = pd.Series(y_test, index = dt_Test.index)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"IeLtl8x92g0g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639234718842,"user_tz":-420,"elapsed":144238,"user":{"displayName":"Le Van Son B1706862","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05760697644039183687"}},"outputId":"5d0126e2-bc3c-49da-d8f2-60908d0bbd5e"},"source":["# kết hợp tập Train và Test lại\n","dt_features = pd.concat([dt_Train,dt_Test])\n","dt_features.shape"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1100000, 2382)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"aqQlfk0nFXan","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"ok","timestamp":1639234908542,"user_tz":-420,"elapsed":165903,"user":{"displayName":"Le Van Son B1706862","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05760697644039183687"}},"outputId":"59ad80d6-1029-43c1-cd72-d4507bcc0d41"},"source":["# Thực hiện hàm create_metadata() để tạo lấy dl cần thiết (lấy sha256)\n","create_metadata(\"/content/ember_2017_2/\")"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sha256</th>\n","      <th>subset</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0abb4fda7d5b13801d63bee53e5e256be43e141faa077a...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>d4206650743b3d519106dea10a38a55c30467c3d9f7875...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7f513818bcc276c531af2e641c597744da807e21cc1160...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ca65e1c387a4cc9e7d8a8ce12bf1bcf9f534c9032b9d95...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1099995</th>\n","      <td>fffe314f23cee3a68ccab272934877d3bc18ec3bd905df...</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>1099996</th>\n","      <td>fffe7a1b23e04facc9ca91a93ac4a34e8b3040e023dbde...</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>1099997</th>\n","      <td>fffe801f51e7ec931515aa49a3d157a9c0fbcdca8c9d80...</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>1099998</th>\n","      <td>fffe92f9593649c4a7050302368189de45e2c1c06b04ea...</td>\n","      <td>test</td>\n","    </tr>\n","    <tr>\n","      <th>1099999</th>\n","      <td>ffffb259a4c5e25ae1437af59caafb718cf8879187cc8c...</td>\n","      <td>test</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1100000 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                    sha256 subset\n","0        0abb4fda7d5b13801d63bee53e5e256be43e141faa077a...  train\n","1        d4206650743b3d519106dea10a38a55c30467c3d9f7875...  train\n","2        c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15...  train\n","3        7f513818bcc276c531af2e641c597744da807e21cc1160...  train\n","4        ca65e1c387a4cc9e7d8a8ce12bf1bcf9f534c9032b9d95...  train\n","...                                                    ...    ...\n","1099995  fffe314f23cee3a68ccab272934877d3bc18ec3bd905df...   test\n","1099996  fffe7a1b23e04facc9ca91a93ac4a34e8b3040e023dbde...   test\n","1099997  fffe801f51e7ec931515aa49a3d157a9c0fbcdca8c9d80...   test\n","1099998  fffe92f9593649c4a7050302368189de45e2c1c06b04ea...   test\n","1099999  ffffb259a4c5e25ae1437af59caafb718cf8879187cc8c...   test\n","\n","[1100000 rows x 2 columns]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"5NO66QcsR0Di","colab":{"base_uri":"https://localhost:8080/","height":261},"executionInfo":{"status":"ok","timestamp":1639234910835,"user_tz":-420,"elapsed":1273,"user":{"displayName":"Le Van Son B1706862","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05760697644039183687"}},"outputId":"35839583-321e-4504-93f5-94e9b240ad76"},"source":["# khởi tạo biến lưu trữ từ metadata.csv \n","emberdf = read_metadata('/content/ember_2017_2/')\n","emberdf.head()"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n","  mask |= (ar1 == a)\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sha256</th>\n","      <th>subset</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0abb4fda7d5b13801d63bee53e5e256be43e141faa077a...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>d4206650743b3d519106dea10a38a55c30467c3d9f7875...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7f513818bcc276c531af2e641c597744da807e21cc1160...</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ca65e1c387a4cc9e7d8a8ce12bf1bcf9f534c9032b9d95...</td>\n","      <td>train</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              sha256 subset\n","0  0abb4fda7d5b13801d63bee53e5e256be43e141faa077a...  train\n","1  d4206650743b3d519106dea10a38a55c30467c3d9f7875...  train\n","2  c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15...  train\n","3  7f513818bcc276c531af2e641c597744da807e21cc1160...  train\n","4  ca65e1c387a4cc9e7d8a8ce12bf1bcf9f534c9032b9d95...  train"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"XJiNwcaKlNSH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639234910837,"user_tz":-420,"elapsed":15,"user":{"displayName":"Le Van Son B1706862","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05760697644039183687"}},"outputId":"d8d9999f-1845-4094-eca8-2e0f466a6237"},"source":["# chuyển type DataFrame trên sang mảng\n","temp = emberdf['sha256'].to_numpy()\n","temp"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['0abb4fda7d5b13801d63bee53e5e256be43e141faa077a6d149874242c3f02c2',\n","       'd4206650743b3d519106dea10a38a55c30467c3d9f78758690a8bbf478e5b6d4',\n","       'c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15b1af16ab9d6cf65065',\n","       ...,\n","       'fffe801f51e7ec931515aa49a3d157a9c0fbcdca8c9d80f942619e98bbdcca23',\n","       'fffe92f9593649c4a7050302368189de45e2c1c06b04ea398d0e0f4f594e81da',\n","       'ffffb259a4c5e25ae1437af59caafb718cf8879187cc8cec61d284345e56e79e'],\n","      dtype=object)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"2dzPKgozWnQb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639234911267,"user_tz":-420,"elapsed":442,"user":{"displayName":"Le Van Son B1706862","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05760697644039183687"}},"outputId":"97f65add-4a8e-4611-932f-edf050a0c844"},"source":["# thêm mảng sha256 vào tập dữ liệu vector vừa thu được\n","dt_features['sha256'] = pd.Series(temp, index = dt_features.index)\n","dt_features.shape"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1100000, 2383)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"IeuLKmU-Awhc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639234911268,"user_tz":-420,"elapsed":10,"user":{"displayName":"Le Van Son B1706862","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05760697644039183687"}},"outputId":"ebd5f141-04b7-467a-b10e-11660c4fd97c"},"source":["# kiểm tra thử sha256 có trùng lập không?\n","mylist = list(dict.fromkeys(dt_features['sha256']))\n","len(mylist)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1100000"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"xG3avyEyIpkq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639234927834,"user_tz":-420,"elapsed":16574,"user":{"displayName":"Le Van Son B1706862","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05760697644039183687"}},"outputId":"9e2700d7-c6a7-4e42-b332-a20309a7b189"},"source":["# Kết nối tới Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"fFfFbihw7bVf","executionInfo":{"status":"ok","timestamp":1639239397203,"user_tz":-420,"elapsed":1478204,"user":{"displayName":"Le Van Son B1706862","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05760697644039183687"}}},"source":["# lưu kết quả DataFrame mới lại thành file (sha256, label, vector đặc trưng)\n","# dt_features.to_csv('metaData_features.csv', index=False)\n","\n","# lưu thẳng vào drive\n","dt_features.to_csv('/content/drive/MyDrive/BIG_Data/metaData_features.csv', index=False)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"pG63EjPU9Vpq"},"source":["# Di chuyển file thu được vào thư mục trong Google Drive \n","# !cp /content/metaData_features.csv /content/drive/MyDrive/BIG_Data/metaData_features.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d2eEk1W-gZDx"},"source":["# lấy những malware có label = 1 / lọc bớt DL lại\n","emberdf_label_1 = dt_features[dt_features.label==1].reset_index(drop = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XIYBfoa8gZZ9"},"source":["# phân chia tập DL ngẫu nhiên\n","fold = 200 \n","from sklearn.model_selection import GroupKFold\n","\n","gkf  = GroupKFold(n_splits = 400)\n","emberdf_label_1['fold'] = -1\n","for fold, (train_idx, val_idx) in enumerate(gkf.split(emberdf_label_1, groups = emberdf_label_1.sha256.tolist())):\n","    emberdf_label_1.loc[val_idx, 'fold'] = fold\n","\n","emberdf_label_1.shape\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHhQ5tiCwFMy"},"source":["emberdf_label_1.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RqYIsEokvvYg"},"source":["emberdf_label_1.to_csv('/content/drive/MyDrive/BIG_Data/metaData_features_label_1.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lLSoyLhVMW4K"},"source":["# lấy ngẫu nhiên các tập nhỏ \n","ember_df1 = emberdf_label_1[emberdf_label_1.fold==2].reset_index(drop = True)\n","ember_df2 = emberdf_label_1[emberdf_label_1.fold==4].reset_index(drop = True)\n","ember_df3 = emberdf_label_1[emberdf_label_1.fold==6].reset_index(drop = True)\n","ember_df4 = emberdf_label_1[emberdf_label_1.fold==8].reset_index(drop = True)\n","ember_df5 = emberdf_label_1[emberdf_label_1.fold==10].reset_index(drop = True)\n","ember_df6 = emberdf_label_1[emberdf_label_1.fold==12].reset_index(drop = True)\n","ember_df7 = emberdf_label_1[emberdf_label_1.fold==16].reset_index(drop = True)\n","ember_df8 = emberdf_label_1[emberdf_label_1.fold==18].reset_index(drop = True)\n","ember_df9 = emberdf_label_1[emberdf_label_1.fold==20].reset_index(drop = True)\n","ember_df10 = emberdf_label_1[emberdf_label_1.fold==22].reset_index(drop = True)\n","ember_df11 = emberdf_label_1[emberdf_label_1.fold==24].reset_index(drop = True)\n","ember_df12 = emberdf_label_1[emberdf_label_1.fold==26].reset_index(drop = True)\n","ember_df13 = emberdf_label_1[emberdf_label_1.fold==27].reset_index(drop = True)\n","ember_df14 = emberdf_label_1[emberdf_label_1.fold==28].reset_index(drop = True)\n","ember_df15 = emberdf_label_1[emberdf_label_1.fold==29].reset_index(drop = True)\n","ember_df16 = emberdf_label_1[emberdf_label_1.fold==32].reset_index(drop = True)\n","ember_df17 = emberdf_label_1[emberdf_label_1.fold==35].reset_index(drop = True)\n","ember_df18 = emberdf_label_1[emberdf_label_1.fold==36].reset_index(drop = True)\n","ember_df19 = emberdf_label_1[emberdf_label_1.fold==37].reset_index(drop = True)\n","ember_df20 = emberdf_label_1[emberdf_label_1.fold==42].reset_index(drop = True)\n","ember_df21 = emberdf_label_1[emberdf_label_1.fold==45].reset_index(drop = True)\n","ember_df22 = emberdf_label_1[emberdf_label_1.fold==52].reset_index(drop = True)\n","ember_df23 = emberdf_label_1[emberdf_label_1.fold==54].reset_index(drop = True)\n","ember_df24 = emberdf_label_1[emberdf_label_1.fold==63].reset_index(drop = True)\n","ember_df25 = emberdf_label_1[emberdf_label_1.fold==62].reset_index(drop = True)\n","ember_df26 = emberdf_label_1[emberdf_label_1.fold==72].reset_index(drop = True)\n","ember_df27 = emberdf_label_1[emberdf_label_1.fold==81].reset_index(drop = True)\n","ember_df28 = emberdf_label_1[emberdf_label_1.fold==82].reset_index(drop = True)\n","ember_df29 = emberdf_label_1[emberdf_label_1.fold==88].reset_index(drop = True)\n","ember_df30 = emberdf_label_1[emberdf_label_1.fold==90].reset_index(drop = True)\n","ember_df31 = emberdf_label_1[emberdf_label_1.fold==91].reset_index(drop = True)\n","ember_df32 = emberdf_label_1[emberdf_label_1.fold==92].reset_index(drop = True)\n","ember_df33 = emberdf_label_1[emberdf_label_1.fold==93].reset_index(drop = True)\n","ember_df34 = emberdf_label_1[emberdf_label_1.fold==94].reset_index(drop = True)\n","ember_df35 = emberdf_label_1[emberdf_label_1.fold==95].reset_index(drop = True)\n","ember_df36 = emberdf_label_1[emberdf_label_1.fold==96].reset_index(drop = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"usyWzXJgjS1P"},"source":["print(\"shape : \", ember_df30.shape)\n","print(ember_df30.head(10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vExp1KKvMZTg"},"source":["# lưu các tập vào Drive \n","ember_df1.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df1.csv\")\n","ember_df2.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df2.csv\")\n","ember_df3.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df3.csv\")\n","ember_df4.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df4.csv\")\n","ember_df5.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df5.csv\")\n","ember_df6.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df6.csv\")\n","ember_df7.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df7.csv\")\n","ember_df8.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df8.csv\")\n","ember_df9.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df9.csv\")\n","ember_df10.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df10.csv\")\n","ember_df11.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df11.csv\")\n","ember_df12.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df12.csv\")\n","ember_df13.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df13.csv\")\n","ember_df14.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df14.csv\")\n","ember_df15.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df15.csv\")\n","ember_df16.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df16.csv\")\n","ember_df17.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df17.csv\")\n","ember_df18.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df18.csv\")\n","ember_df19.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df19.csv\")\n","ember_df20.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df20.csv\")\n","ember_df21.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df21.csv\")\n","ember_df22.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df22.csv\")\n","ember_df23.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df23.csv\")\n","ember_df24.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df24.csv\")\n","ember_df25.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df25.csv\")\n","ember_df26.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df26.csv\")\n","ember_df27.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df27.csv\")\n","ember_df28.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df28.csv\")\n","ember_df29.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df29.csv\")\n","ember_df30.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df30.csv\")\n","ember_df31.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df31.csv\")\n","ember_df32.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df32.csv\")\n","ember_df33.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df33.csv\")\n","ember_df34.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df34.csv\")\n","ember_df35.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df35.csv\")\n","ember_df36.to_csv(\"/content/drive/MyDrive/Data_malware/ember/df36.csv\")"],"execution_count":null,"outputs":[]}]}